# フロアプラン LoRA トレーニングガイド

このディレクトリには、LoRA（Low-Rank Adaptation）を使用して Stable Diffusion 2.1 をフロアプラン生成用にファインチューニングするために必要なすべてが含まれています。

## 概要

トレーニングプロセスは、80枚のフロアプラン画像のデータセットでモデルをファインチューニングし、特定のスタイルに合った建築図面を生成します。

## クイックスタート

1. **依存関係のインストール**（まだの場合）:
   ```bash
   pip3 install -r requirements.txt
   ```

2. **トレーニング開始**:
   ```bash
   ./start_training.sh
   ```

## トレーニングプロセス

### 1. データセット準備
- `prepare_dataset.py` - フロアプラン画像を処理しメタデータを作成
- 訓練/検証の分割（90/10）
- フロアタイプとグリッドサイズに基づいて各画像のキャプションを生成

### 2. LoRA トレーニング
- `train_lora_simple.py` - M2 Max 向けに最適化されたメイントレーニングスクリプト
- メモリ効率のため LoRA ランク 4 を使用
- バッチサイズ 1 で 5 エポック訓練
- 100ステップごとにチェックポイントを保存

### 3. モデル出力
- チェックポイントは `lora_model/checkpoint-*` に保存
- 最終モデルは `lora_model/final/` に保存
- トレーニング設定は `lora_model/adapter_config.json` に保存

## トレーニング設定

主要パラメータ（`train_lora_simple.py` で変更可能）:
- **エポック数**: 5（品質向上のため増やすことも可能）
- **バッチサイズ**: 1（GPU メモリに制限）
- **学習率**: 5e-5
- **LoRA ランク**: 4（低い = メモリ使用量少、高い = 表現力向上）
- **ターゲットモジュール**: ["to_k", "to_q", "to_v", "to_out.0"]

## 予想される所要時間

M2 Max（64GB）の場合:
- データセット準備: 約1分
- トレーニング: 1-2時間
- エポックあたり: 15-25分

## メモリ要件

- 最小: 16GB RAM
- 推奨: 32GB以上の RAM
- GPU: M2/M3 Max または 8GB以上の VRAM を持つ NVIDIA GPU

## トレーニング済みモデルの使用

トレーニング後、バックエンドを LoRA モデルを使用するように更新:

1. `backend/src/index.ts` を編集:
   ```typescript
   // 変更前:
   const pythonScriptPath = path.join(__dirname, '../../inference/simple_floor_plan.py');
   // 変更後:
   const pythonScriptPath = path.join(__dirname, '../../inference/generate_floor_plan_lora.py');
   ```

2. 推論スクリプトは自動的に `training/lora_model/final/` から LoRA ウェイトを読み込みます

## トレーニングの監視

トレーニングの進捗はターミナルに表示されます:
- 損失値（低いほど良い）
- 各エポックの進捗バー
- チェックポイントの保存

## トラブルシューティング

1. **メモリ不足**: バッチサイズまたは LoRA ランクを削減
2. **トレーニングが遅い**: CPU では通常、可能なら GPU を使用
3. **損失が高い**: エポック数を増やすか学習率を調整

## 高度なオプション

より詳細な制御のため、`train_lora_simple.py` を直接編集:
- 画像サイズの調整（デフォルト: 512x512）
- トレーニングステップ数の変更
- LoRA 設定の変更
- トラッキング用の wandb ロギングの追加

## 結果

トレーニング後、モデルは以下を生成できるようになります:
- データセットに一致する一貫したスタイルのフロアプラン
- 建築要素のより良い理解
- より一貫性のある部屋のレイアウト
- よりクリーンな線画

品質は以下により向上します:
- より多くのトレーニングエポック
- より大きなデータセット
- より高い LoRA ランク（メモリが許す場合）